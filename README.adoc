= Data engineering with AWS - Final Project
:stylesheet: boot-darkly.css
:linkcss: boot-darkly.css
:image-url-ironhack: https://user-images.githubusercontent.com/23629340/40541063-a07a0a8a-601a-11e8-91b5-2f13e4e6b441.png
:my-name: Jorge Castro DAPT NOV2021
:description:
:script-url: 
//:fn-xxx: Add the explanation foot note here bla bla
:toc:
:toc-title: 
:toc-placement!:
:toclevels: 5
ifdef::env-github[]
:sectnums:
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
:experimental:
:table-caption!:
:example-caption!:
:figure-caption!:
:idprefix:
:idseparator: -
:linkattrs:
:fontawesome-ref: http://fortawesome.github.io/Font-Awesome
:icon-inline: {user-ref}/#inline-icons
:icon-attribute: {user-ref}/#size-rotate-and-flip
:video-ref: {user-ref}/#video
:checklist-ref: {user-ref}/#checklists
:list-marker: {user-ref}/#custom-markers
:list-number: {user-ref}/#numbering-styles
:imagesdir-ref: {user-ref}/#imagesdir
:image-attributes: {user-ref}/#put-images-in-their-place
:toc-ref: {user-ref}/#table-of-contents
:para-ref: {user-ref}/#paragraph
:literal-ref: {user-ref}/#literal-text-and-blocks
:admon-ref: {user-ref}/#admonition
:bold-ref: {user-ref}/#bold-and-italic
:quote-ref: {user-ref}/#quotation-marks-and-apostrophes
:sub-ref: {user-ref}/#subscript-and-superscript
:mono-ref: {user-ref}/#monospace
:css-ref: {user-ref}/#custom-styling-with-attributes
:pass-ref: {user-ref}/#passthrough-macros
endif::[]
ifndef::env-github[]
:imagesdir: ./
endif::[]

image::{image-url-ironhack}[width=70]

{my-name}


                                                     
====
''''
====
toc::[]

{description}


= About the project:

In essence, I want to mimic a e-Commerce platform to make sure that once the data is coming in, the data is getting processed, stored then be accessible to be used and visualized.

I am going to build:  

* a data ingestion pipeline, with API, Lambda and Kenisis.
* a streaming pipeline to raw data on S3.
* a storage element to DynamoDB (NoSQL) and a visualization of this data with an API.
* Streaming data to a data warehouse with Redshift.
* a batch processing pipeline for bulk import.

== Dataset

https://www.kaggle.com/datasets/carrie1/ecommerce-data[E-Commerce Data]

Actual transactions from UK retailer

From https://www.kaggle.com/datasets/carrie1/ecommerce-data

CSV files

Customer ID
Invoice number 
Description of the item
invoice date
Unit price and quantity field

540.000 rows



== Main goal:

Build simple streaming pipeline to learn how AWS API Gateway / AWS Lambda works together.


: User / Business

* Transactions will be started in the e-Commerce platform, then this transactions must be, stored in a single location.

* Customers need access to the data (purchases history)


Secondary goal: Business intelligence goals
This are things that make optimize the business

For example:

Average sales per hour/day/month/year

Most sold product h/m/y

Highest invoices

Customer insights

== Used Tools

* Which tools and why

* How do the tools work

* Why did I choose them

* How did I set them up

== Relational storage vs Non-relational storage

This is a snapshot of the data we are going to deal with:


image::https://user-images.githubusercontent.com/63274055/167107915-9c1db302-e446-433f-816f-ed4eb681dbc6.png[width=600]

At first glance, I would say that this is a very structured and simple dataset, it makes sense to store it in a structured way, in a relational database. It would take maximum four tables. Let#s say, Customer, Invoice, Stock and a table to help us to create a relationship between Invoice number + Stock codeID. This database will then extract the data, transform it and send it to us by a result.

On a small scale this setup is totally valid, however if we think of having millions of customers, all processing the data, specially when writing the data as invoices are being created all the time with many writes, needing to go fast, with every import indexes would need to be created, this setup would create extra levels of complexity and slowing down the platform.

What I rather use here is a Non-relational wide column store.

== Platform Design

image::https://user-images.githubusercontent.com/63274055/167119770-8d0d992a-3627-4a1d-8286-b2686dd64dbc.png[width=800]


This is the blueprint of what I am going to build.

The Client is the way to actually process the data, which is going to be on my pc with the CSV file stored.there.

Python will take lines out of the CSV file and send them over to the Connector, the API Gateway as a JSON string.

Then we have Kinesis as a Buffer.

For the  Processing Framework I will use Lambda functions and when doing batch processing I'll use CloudWatch to actually trigger the Lambdas.

As storage, I will opt for multiple options:

Raw data is going to live on S3, everything coming in will be sent to S3. Data will be in a Non-SQL store, DynamoDB, the I will use Redshift as my data Warehouse.

In terms of how I am going to visualize the data I opted for Tableau, with an API specially for visualization.

image::https://github.com/jecastrom/ironhack-final-project/blob/main/pic/pipelines.jpg[width=800]

=== Client

Setting up a Client to send the data:

image::https://user-images.githubusercontent.com/63274055/167126585-fea1d925-62df-42bf-97cd-b9a62a4efa9e.png[width=800]

=== Connect

image::https://user-images.githubusercontent.com/63274055/167153132-d42160c2-3238-4512-991c-c911275bc5e3.png[width=800]

In this Connect face we have the Client which is sending data to our API Gateway that is hosting a URL.

When the data is sent to the API, in the background is living a Lambda function that is going to get triggered by the API Gateway and that is processing the JSON we have and it is going to access a Buffer and send it into Kinesis. 

=== Buffer

image::https://user-images.githubusercontent.com/63274055/167160182-a5625423-90fc-49e2-b624-24b3315769b9.png[width=800]

Here we have Kinesis in the middle, a message queue. The way message queues work is that we have Producers and we have Consumers. The Producer is going to send data into the Message Queue and the Consumer is going to take data out of the Message Queue. In my case, the Producer is the Lambda Function that sits behind the API Gateway. 

The Lambda Function is getting triggered by the API Gateway and it is going to send or produce the data string for Kinesis. So every message or every transaction or row that is coming in from the dataset that has been processed into JSON is going to be added into Kinesis. Finally we can have either a Lambda Function or we could have a Kinesis Firehose which will take the data back out.

=== Process

image::https://user-images.githubusercontent.com/63274055/167196783-afbbef55-fe3e-4019-a9da-6de1ea264188.png[width=800]


The are two ways of processing data. We can either do stream processing or batch processing.
When we think about Stream Processing we have a Source (Kinesis) and its sending data into 
Processing (Lambda Function which is triggered by a new Kinesis record), once new data is written 
into Kinesis the Lambda Function automatically runs and processes the data right away and puts it 
into a destination.

Batch Processing in another hand starts with the Scheduler. Once the data is put into the data 
source (S3 Kinesis). A Scheduler(CloudWatch) is going to start and activate the Processing 
(Lambda Function). The Processing is going to connect to the data source, pull the data,
process the data then write the data to the destination.

=== Storage

image::https://user-images.githubusercontent.com/63274055/167223592-e36eb180-ea7e-4d74-9f7f-1acde7339a87.png[width=600]

I am going to use S3 file storage, which is very simple to use and I will use it for bulk imports when we have transactions coming in as a file then we upload them to S3, triggering a bulk import then write it directly into DynamoDB and the Redshift Data Warehouse. I will use DynamoDB wide column store which is a non-relational database. 

I will use it because for this business application I want to simulate in this project, in my opinion, it does not make sense to use a SQL database. DynamoDB is for the backend where the transactions are going to happen and where the "customer" or user is going to visualize their data, in this case their invoices and the invoice detail.

This project has a primary use case which is for the actual business and the user, then I have an Analytics use case which is secondary. For this analytics layer which is on top of the actual database layer, Redshift will be my data warehouse. 

This is to simulate what big companies do in this situations, having databases all over the company and they want to use analytics on top of this databases, so they pull data from this databases up into an analytics layer (into a data warehouse) where thy have then a storage and a way to accessing the data. This is not for transactions or business related, it is actually a duplication of the data.

=== Visualize

* API's
** Access for Apps, UIs.
** Execute queries and transaction

* Tableau
** Business Intelligence tool
** Installed on pc
** Connects to Redshift

== Planning the Data Pipelines

Pipelines are the main vehicle that makes data science happen. Pipelines make sure that the data flows from the moment data gets in (Ingestion) through the whole platform and make the data accessible to other systems, data analysts, data scientist and internal users.

=== Data Ingestion Pipeline for high ingestion loads

image::https://user-images.githubusercontent.com/63274055/167260893-0038d3ed-83af-425e-a66c-ed5b188c04bd.png[width=600]

I am going to create a Client that simulates the streaming of the data. It has the CSV file from Kaggle.
Then my aim is to send in rows from the CSV file as JSON into the url of the API gateway.

The Client is going to basically take each row of the CSV file, convert it into a JSON object, then adds that JSON object (each row of the CSV file) into the body of the HTTP post, then post it to the API.

Once it is on the API gateway, where the URL is hosted, there is going to be a Lambda Function triggered, then this Lambda Function is going to take the body of tke post and it is going to write it into Kinesis.

=== Stream to S3 Raw Storage Pipeline

image::https://user-images.githubusercontent.com/63274055/167261665-f99cff9d-fe81-40ad-b7ac-34f6e2a0020c.png[width=600]

The idea here is to take the data that is in Kinesis Stream, trigger the Lambda with it each and every time something is inserted into the Kinesis Data Stream then puts the data into a S3 bucket as a file.

The main use case of this pipeline is to put data into a datalake so we can later on use this data in different processes.


=== Stream to DynamoDB Pipeline

image::https://user-images.githubusercontent.com/63274055/167268945-5d7be067-3a2f-4388-b92e-9d5539e780a5.png[width=600]

This pipeline is going to stream the data from Kinesis into DynamoDB.

Having the data buffered into Kinesis, the data needs to be processed and sent to DynamoDB. This pipeline is similar to the previous one, I am going to take the Kinesis stream data (messages), trigger a Lambda Function with it each time it comes in, and this Lambda Function then re-processes and re-formats the data according to how I am going to then query it later.

=== Visualization Pipeline API


image::https://user-images.githubusercontent.com/63274055/167271582-c47599d6-1227-455f-b5f3-761de17f3d8f.png[width=600]


Here I will build an API to query items from an invoice.

On the left side the data resides in DynamoDB Invoices Table and on the right side is where the Client sits. The Client could be for example an app, an UI. So the Client makes a call onto the invoice API and tells it basically that is has an invoice number, now tell me all the items. Then the request gets processed by a Lambda Function which is going to look into DynamoDB, take the data and return it to the Client. The way I will do this is by sending the invoice number in the request parameter.

=== Visualization Pipeline Redshift Data Warehouse

image::https://user-images.githubusercontent.com/63274055/167290091-e1b30c46-3111-40b7-9d66-31be449e6d28.png[width=600]

AWS Kinesis Firehose Delivery Stream is ideal to process data in Kinesis. Firehose Delivery Stream basically docks on the Kinesis Data Stream and they can process the data further. Then the Delivery Stream automatically delivers the data to the Redshift Data Warehouse with not extra processing.

So I am going to connect the Firehose to the Stream and the Firehose then writes the data into an intermediate S3 bucket. Once there in S3 Kinesis Firehose will trigger a copy to Redshift Function and that copies the complete content of the files into the Redshift table. Once the data is on Redshift we can connect to Tableau and access the data. 

=== Batch Processing Pipeline

image::https://user-images.githubusercontent.com/63274055/167305636-a5c61110-548c-4cd2-b02e-40e2cb75ceaa.png[width=600]

To be able to handle those cases when hundreds of Gb of data need to be uploaded, feeding this data to the Stream Processing would be inefficient. Instead we can do this with a bulk import. So we would simply store the file into S3, CloudWatch would trigger a Lambda Function which takes the data and write it into the DynamoDB tables and into Redshift.

== Building the Data Ingestion Pipeline

=== Creating the Lambda Function for API:

Creating a Lambda Function with a new role to give it permission to write into Kinesis.

image::https://user-images.githubusercontent.com/63274055/167586007-b2bc62cf-5ea0-45eb-99fb-fb93253d92d8.png[width=600]


=== Creating API Gateway

image::https://user-images.githubusercontent.com/63274055/167670433-6d03d9ab-5085-40be-b63a-ffa46db139cb.png[width=600]


On my AWS account, I selected API Gateway => Create API => Rest API => Build

Then select settings:

* Choose the protocol: REST
* Create new API: New API
* Settings:
** API Name: myapi
** Endpoint Type: Regional (Regional API are deployed in the current AWS region)

Then Save API.

On resources Action I have to select two actions:

* Create Resource: 
** Resource Name: hello
** Resource Path: hello

Click on Create Resource

Second Resource Action I select is Create Method:

* Next I add three methods: GET, POST, PUT and configure the Lambda Function (WriteKinesis) in each Method by going to Integration Request.

Also in order for the Lambda Function to receive the body and the requests parameter I had to go into Integration Request again and configure Mapping Templates. Click on Add mapping Template and type `application/json`.

Then in Generate Template I selected `Method request passthrough`



image::https://user-images.githubusercontent.com/63274055/167672051-17849b3d-09bf-441c-8ff5-befbb5ce2671.png[width=600]


=== Setting up Kinesis Data Stream

image::https://github.com/jecastrom/ironhack-final-project/blob/main/pic/kdatastream.gif[width=600]

=== Setup IAM for API

Here we make sure that the Lambda Function can actually write into Kinesis. 

Added IAM policies to the Lambda Function:
* AmazonKinesisFullAccess
** Created a new policy called `MyKinesisWriteApiData` and added:
** Kinesis => Write => PutRecord and PutRecords ==> Resources: All resources then add new policy to the WriteKinesis role.

** Created new policy for the Lambda Function called `myGetDynamoDB` and added:
** DynamoDB => Read => GetItem => All resources

=== Create Ingestion Pipeline (Code)

```python
import json
import boto3

def lambda_handler(event, context):

    print("MyEvent:")
    print(event)

#    mycontext = event.get("context")
#    method = mycontext.get("http-method")
    method = event['context']['http-method']

# With this if else statement we are saying if the method that is 
# coming in is a GET method then in the later # stages we use DynamoDB



    if method == "GET":
        # TODO: write code...
        dynamo_client = boto3.client('dynamodb')

        im_customerID = event['params']['querystring']['CustomerID']
        print(im_customerID)
        response = dynamo_client.get_item(TableName = 'Customers', Key = {'CustomerID':{'N': im_customerID}})
        print(response['Item'])

        #myreturn = "This is the return of the get"

        return {
            'statusCode': 200,
            'body': json.dumps(response['Item'])
           }

# If the method is POST we send in our data. From the "event" 
# we extract the body JSON, take this p_record string then we dump 
# it into a recordstring variable.

    elif method == "POST":

#       mystring = event['params']['querystring']['param1']
        p_record = event['body-json']
        recordstring = json.dumps(p_record)

# We create a client for Kinesis, put a record into Kinesis for the APiData stream.

        client = boto3.client('kinesis')
        response = client.put_record(
            StreamName='APIData',
            Data= recordstring,
            PartitionKey='string'
        )

        return {
            'statusCode': 200,
            'body': json.dumps(p_record)
        }
    else:
        return {
            'statusCode': 501,
            'body': json.dumps("Server Error")
        }
```

=== Create Script to Send Data

```python
import pandas as pd
import requests


# URL of our endpoint. After deploying the API and creating a Stage we 
# can see the URL of the stage we are in. In this case this is the POST URL.
URL = "https://2krjwwbp8d.execute-api.us-east-1.amazonaws.com/prod/hello"


#read the testfile with Pandas then goes into a DataFrame
data = pd.read_csv('data/TestSample.csv', sep = ',')

# write a single row from the testfile into the api
#export = data.loc[2].to_json()
#response = requests.post(URL, data = export)
#print(response)

# write all the rows from the testfile to the api as put request

# Looping over the index of the data in the DataFrame and use 
# every one of this lines, put them into a JSON then use it as an 
# export for the URL
for i in data.index:
    try:
        # convert the row to json
        export = data.loc[i].to_json()

        #send it to the api
        response = requests.post(URL, data = export)

        # print the returncode
        print(export)
        print(response)
    except:
        print(data.loc[i])
```

=== Testing the Data Ingestion Pipeline



image::https://github.com/jecastrom/ironhack-final-project/blob/main/pic/test%20dip.gif[width=900]

== Building the Stream To Raw S3 Storage Pipeline

=== Setup S3 Bucket

image::https://user-images.githubusercontent.com/63274055/167963196-a814c399-e63d-426b-aadd-82d3c5a51014.png[width=600]

=== Configure IAM for S3

Created a IAM role for a Lambda Function and added policies to it. 

image::https://user-images.githubusercontent.com/63274055/168026755-35df6ea2-3f63-43bf-94ff-61dfc3edd2ef.png[width=600]

=== Creating Lambda For S3 Insert

* Created Lambda Function from Blueprint `kinesis-process-record-python`. 
* Added existing role `Lambda-Kinesis-S3-Writer`
* The Kinesis trigger is the previous Kinesis Data Stream I named `APIData`.
* Batch size determines how many lines of data will be on every file. I left it to 100 lines.

The Lambda code:

```python
from __future__ import print_function

import base64
# We need base64 as the messages from Kinesis are base64 encoded
import json
# boto3 to connect to S3
import boto3
from datetime import datetime

# Creating a boto3 Client for S3
s3_client = boto3.client('s3')

# Converting datetime object to string
dateTimeObj = datetime.now()

# formatting the string
timestampStr = dateTimeObj.strftime("%d-%b-%Y-%H%M%S")

# Creating a list for Kinesis records
kinesisRecords = []

# Function to process the incoming events from Kinesis or lines of data 
# I chose when setting up the Lambda Function
def lambda_handler(event, context):
    #print("Received event: " + json.dumps(event, indent=2))
    for record in event['Records']:
        # Kinesis data is base64 encoded so here we encode it.
        # If we run into the error: [ERROR] TypeError: sequence item 0: expected str instance, bytes found
        # then we add the encoding into UTF8: 
        #payload = base64.b64decode(record['kinesis']['data']).decode('utf-8')
        payload = base64.b64decode(record['kinesis']['data'])


        # appending each record to a list
        kinesisRecords.append(payload)
        # this is just for logging
        # print("Decoded payload: " + payload)

    # making a string out of the list. Backslash n for new line in the s3 file
    ex_string = '\n'.join(kinesisRecords)

    # generate the name for the file with the timestamp
    mykey = 'output-' + timestampStr + '.txt'

    #putting the file into the s3 bucket
    response = s3_client.put_object(Body=ex_string, Bucket='stream-data-lake', Key= mykey)
    # returning how many records have been processed within the function
    return 'Successfully processed {} records.'.format(len(event['Records']))
```
=== Testing the Stream To Raw S3 Storage Pipeline

image::https://github.com/jecastrom/ironhack-final-project/blob/main/pic/stream%20to%20s3%20pp.gif[width=800]


== Building the Stream To DynamoDB Pipeline

=== Setting Up DynamoDB

On AWS DynamoDB I created two tables

* Customers
* Invoices

image::https://user-images.githubusercontent.com/63274055/168077305-1a5a1bda-e36c-4342-b299-41461885fc2b.png[width=600]

=== Setting up IAM For DynamoDB Stream


image::https://user-images.githubusercontent.com/63274055/168082126-06ee21e2-5996-4a15-b3dd-5b5bfe34026d.png[width=600]


=== Creating the DynamoDB Lambda

```python
import json
import base64
import boto3

from datetime import datetime


def lambda_handler(event, context):
    # Creating boto3 client for DynamoDB
    client = boto3.client('dynamodb')

    #print("Received event: " + json.dumps(event, indent=2))
    for record in event['Records']:

        # Kinesis data is base64 encoded so decode here
        t_record = base64.b64decode(record['kinesis']['data'])

        # decode the bytes into a string
        str_record = str(t_record, 'utf-8')

        # transform the json string into a dictionary
        dict_record = json.loads(str_record)

        # create Customer Row
        ############################

        customer_key = dict()
        customer_key.update(
            {'CustomerID': {"N": str(dict_record['CustomerID'])}})
        # Text that is going to be written in the invoice number
        ex_customer = dict()
        ex_customer.update({str(dict_record['InvoiceNo']): {
                           'Value': {"S": 'Invoice'}, "Action": "PUT"}})

        response = client.update_item(
            TableName='Customers', Key=customer_key, AttributeUpdates=ex_customer)

        # Create Inventory Row
        #############################
        # Creating a dictionary for the invoice to get the invoice number
        inventory_key = dict()
        inventory_key.update(
            {'InvoiceNo': {"N": str(dict_record['InvoiceNo'])}})

        # create export dictionary
        ex_dynamoRecord = dict()

        # remove Invoice and Stock code from dynmodb record
        stock_dict = dict(dict_record)
        stock_dict.pop('InvoiceNo', None)
        stock_dict.pop('StockCode', None)

        # turn the dict into a json
        stock_json = json.dumps(stock_dict)

        # create a record (column) for the InvoiceNo
        # add the stock json to the column with the name of the stock number
        ex_dynamoRecord.update({str(dict_record['StockCode']): {
                               'Value': {"S": stock_json}, "Action": "PUT"}})

        # print(ex_dynamoRecord)
        response = client.update_item(
            TableName='Invoices', Key=inventory_key, AttributeUpdates=ex_dynamoRecord)

    return 'Successfully processed {} records.'.format(len(event['Records']))

```
=== Testing the Stream To DynamoDB Pipeline

image::https://github.com/jecastrom/ironhack-final-project/blob/main/pic/dynamodb%20pipe.gif[width=900]

== Building the Visualization API

This API is to visualize the data residing in DynamoDB:

I created the new lambda called `Get-data-from-DynamoDB`, same trigger and got assigned 
a role automatically. I added a new policy called "myGetDynamoDB" with permissions to read 
DynamoDB => GetItem to that new role.  On the API helloworld, on the GET method I changed the 
lambda function in the integration request to the new one `Get-data-from-DynamoDB`. 

Invoke URL: https://2krjwwbp8d.execute-api.us-east-1.amazonaws.com/prod/hello


```python
import json
import boto3


def lambda_handler(event, context):

    print("MyEvent:")
    print(event)

#    mycontext = event.get("context")
#    method = mycontext.get("http-method")
    method = event['context']['http-method']

    if method == "GET":
        # todo write code...
        dynamo_client = boto3.client('dynamodb')

        im_invoiceID = event['params']['querystring']['InvoiceNo']
        print(im_invoiceID)
        response = dynamo_client.get_item(TableName='Invoices', Key={
                                          'InvoiceNo': {'N': im_invoiceID}})
        print(response['Item'])

        #myreturn = "This is the return of the get"

        return {
            'statusCode': 200,
            'body': json.dumps(response['Item'])
        }

    elif method == "POST":

        #       mystring = event['params']['querystring']['param1']
        p_record = event['body-json']
        recordstring = json.dumps(p_record)

        client = boto3.client('kinesis')
        response = client.put_record(
            StreamName='APIData',
            Data=recordstring,
            PartitionKey='string'
        )

        return {
            'statusCode': 200,
            'body': json.dumps(p_record)
        }
    else:
        return {
            'statusCode': 501,
            'body': json.dumps("Server Error")
        }

```
=== Testing the Visualization API with Postman

https://2krjwwbp8d.execute-api.us-east-1.amazonaws.com/prod/hello?InvoiceNo=536365

image::https://github.com/jecastrom/ironhack-final-project/blob/main/pic/visual%20dydb%20api.gif[width=900]


== Building the Visualization Pipeline Redshift Data Warehouse

=== Setting up Redshift Data Warehouse


image::https://user-images.githubusercontent.com/63274055/168296872-37d1690b-03ce-47e5-a5d6-e1310583fe02.png[width=600]

=== Setting up VPC security Group for Firehose

To make sure that Firehose can actually send data into Redshift we add an Inbound Rule allowing access to the IP address of Firehouse in my region 52.70.63.192/27.

Created an Elastic IP and made the Redshift Cluster publicly accessible.

image::https://user-images.githubusercontent.com/63274055/168301607-ecf54b9c-01e2-45c7-9923-64fb344e0233.png[width=600]

=== Creating Redshift Tables

In the Redshift query editor I created the table:

```sql
create table firehosetransactions(
	InvoiceNo varchar(200) not null,
	StockCode varchar(200) not null,
	Description varchar(200) not null,
	Quantity int not null,	
	InvoiceDate varchar(200) not null,
	UnitPrice float not null,
	CustomerID int not null,  	
 	Country varchar(200) not null
);
```
image::https://user-images.githubusercontent.com/63274055/168320249-f727e567-e3e5-4180-98aa-e5c0206a858c.png[width=600]



=== Bucket and jsonpaths.json

Created new S3 Bucket called `data-firehose-to-redshift`. we need to help `Firehose` to make the transformation from the `JSON` we are putting in into the table in `Redshift`. This is because `Firehose` does not know how are the objects called in the `JSON` string; example CustomerID etc.

So we need to add this guide on a `JSON` file on the S3 so that the incoming data can be detected.

```json
{
  "jsonpaths": [
      "$['InvoiceNo']",
      "$['StockCode']",
      "$['Description']",
      "$['Quantity']",
      "$['InvoiceDate']",
      "$['UnitPrice']",
      "$['CustomerID']",
      "$['Country']"
    ]
}
```
image::https://user-images.githubusercontent.com/63274055/168334686-c0096b72-8d2c-4eda-a3ef-88b7ccd6ed66.png[width=600]

=== Configuring Firehose

* Copy command from S3 to Redshift

```
COPY firehosetransactions FROM 's3://data-firehose-to-redshift/<manifest>' CREDENTIALS 'aws_iam_role=arn:aws:iam::<aws-account-id>:role/<role-name>' MANIFEST json 's3://data-firehose-to-redshift/jsonpaths.json';
```

Set the buffer interval to 60 sec so I can see the logs sooner for testing.

image::https://user-images.githubusercontent.com/63274055/168382099-a09cbf60-90ec-4c2e-897e-90a49a3dca9f.png[width=600]

=== Testing the Visualization Pipeline Data Warehouse writing to Redshift

image::https://github.com/jecastrom/ironhack-final-project/blob/main/pic/test%20redshift%20records.gif[width=800]

Streaming test results:

Records written to Redshift: 98/1000

Records written to DynamoDB: 46/1000

Records written on S3 s3-data-lake-project: non



=== Debugging Redshift Streaming



* From the API Gateway side: Debug
** Is the data coming into the API gateway?
** Is data coming into Kinesis?
** Is data coming into the S3 folder of Firehose?
** Is the copy of data being inserted into Redshift working?

Starting from the Redshift side:

To see insert errors on Redshift: query Table `stl_load_errors`

== Building a Batch Processing Pipeline with Glue

Here I have done batch processing with AWS Glue on a Bulk Import Pipeline. This is useful in cases when we get a big file and import it into Redshift. Basically we create a job that takes the data from S3 and writes it into Redshift.

AWS Glue is an ETL service that takes data from somewhere, extracts it, transforms it, then it loads it elsewhere. 

Glue also maps the structure of the data stored in the data source and creates a Glue Catalog.


* Create a new table on Redshift that I called `bulkimport`

```sql
create table bulkimport(
	InvoiceNo varchar(200) not null,
	StockCode varchar(200) not null,
	Description varchar(200) not null,
	Quantity int not null,	
	InvoiceDate varchar(200) not null,
	UnitPrice float not null,
	CustomerID int not null,  	
 	Country varchar(200) not null
);
```


* In AWS Glue I create a Glue database I called `glue-transactionsdb`
image::https://user-images.githubusercontent.com/63274055/168492814-f041f7b2-05d7-481a-9009-281c4ecc7438.png[width=600]

*Before creating the crawlers I had to set the IAM policies

** Created a role for  and added the policies:

image::https://user-images.githubusercontent.com/63274055/168493736-767097bd-a90b-44c6-a7a0-b15cf126a780.png[width=600]

** Created an VPC Endpoint to S3 so the Glue job can reach S3 out of the VPC

image::https://user-images.githubusercontent.com/63274055/168561528-04f473ca-4ba0-42a6-80d3-5f480579b9f6.png[width=600]

* I create two crawlers that are going to crawl the data sources, one crawler for the bulkImport folder and one crawler for Redshift. This crawlers are going to populate the table within the data catalog.

** `BulkImportCrawler`
** `RedshiftTransactionsCrawler`

image::https://user-images.githubusercontent.com/63274055/168495470-dcba5eb2-3a0d-4709-a21f-81147d7a4471.png[width=600]
====
''''
====


//{script-url}[Solutions script only]

xref:Lab-xxxx[Top Section]

xref:Last-section[Bottom section]

//bla bla blafootnote:[{fn-xxx}]


////
.Unordered list title
* gagagagagaga
** gagagatrtrtrzezeze
*** zreu fhjdf hdrfj 
*** hfbvbbvtrtrttrhc
* rtez uezrue rjek  

.Ordered list title
. rwieuzr skjdhf
.. weurthg kjhfdsk skhjdgf
. djhfgsk skjdhfgs 
.. lksjhfgkls ljdfhgkd
... kjhfks sldfkjsdlk




[,sql]
----
----



[NOTE]
====
A sample note admonition.
====
 
TIP: It works!
 
IMPORTANT: Asciidoctor is awesome, don't forget!
 
CAUTION: Don't forget to add the `...-caption` document attributes in the header of the document on GitHub.
 
WARNING: You have no reason not to use Asciidoctor.

bla bla bla the 1NF or first normal form.footnote:[{1nf}]Then wen bla bla


====
- [*] checked
- [x] also checked
- [ ] not checked
-     normal list item
====
[horizontal]
CPU:: The brain of the computer.
Hard drive:: Permanent storage for operating system and/or user files.
RAM:: Temporarily stores information the CPU uses during operation.






bold *constrained* & **un**constrained

italic _constrained_ & __un__constrained

bold italic *_constrained_* & **__un__**constrained

monospace `constrained` & ``un``constrained

monospace bold `*constrained*` & ``**un**``constrained

monospace italic `_constrained_` & ``__un__``constrained

monospace bold italic `*_constrained_*` & ``**__un__**``constrained

////
