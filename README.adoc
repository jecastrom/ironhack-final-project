= Data engineering with AWS - Final Project
:stylesheet: boot-darkly.css
:linkcss: boot-darkly.css
:image-url-ironhack: https://user-images.githubusercontent.com/23629340/40541063-a07a0a8a-601a-11e8-91b5-2f13e4e6b441.png
:my-name: Jorge Castro DAPT NOV2021
:description:
:script-url: 
//:fn-xxx: Add the explanation foot note here bla bla
:toc:
:toc-title: 
:toc-placement!:
:toclevels: 5
ifdef::env-github[]
:sectnums:
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
:experimental:
:table-caption!:
:example-caption!:
:figure-caption!:
:idprefix:
:idseparator: -
:linkattrs:
:fontawesome-ref: http://fortawesome.github.io/Font-Awesome
:icon-inline: {user-ref}/#inline-icons
:icon-attribute: {user-ref}/#size-rotate-and-flip
:video-ref: {user-ref}/#video
:checklist-ref: {user-ref}/#checklists
:list-marker: {user-ref}/#custom-markers
:list-number: {user-ref}/#numbering-styles
:imagesdir-ref: {user-ref}/#imagesdir
:image-attributes: {user-ref}/#put-images-in-their-place
:toc-ref: {user-ref}/#table-of-contents
:para-ref: {user-ref}/#paragraph
:literal-ref: {user-ref}/#literal-text-and-blocks
:admon-ref: {user-ref}/#admonition
:bold-ref: {user-ref}/#bold-and-italic
:quote-ref: {user-ref}/#quotation-marks-and-apostrophes
:sub-ref: {user-ref}/#subscript-and-superscript
:mono-ref: {user-ref}/#monospace
:css-ref: {user-ref}/#custom-styling-with-attributes
:pass-ref: {user-ref}/#passthrough-macros
endif::[]
ifndef::env-github[]
:imagesdir: ./
endif::[]

image::{image-url-ironhack}[width=70]

{my-name}


                                                     
====
''''
====
toc::[]

{description}


= About the project:

In essence, I want to mimic a e-Commerce platform to make sure that once the data is coming in, the data is getting processed, stored then be accessible to be used and visualized.

I am going to build:  

* a data ingestion pipeline, with API, Lambda and Kenisis.
* a streaming pipeline to raw data on S3.
* a storage element to DynamoDB (NoSQL) and a visualization of this data with an API.
* Streaming data to a data warehouse with Redshift.
* a batch processing pipeline for bulk import.

== Dataset

https://www.kaggle.com/datasets/carrie1/ecommerce-data[E-Commerce Data]

Actual transactions from UK retailer

From https://www.kaggle.com/datasets/carrie1/ecommerce-data

CSV files

Customer ID
Invoice number 
Description of the item
invoice date
Unit price and quantity field

540.000 rows



Main goal
: User / Business

* Transactions will be started in the e-Commerce platform, then this transactions must be, stored in a single location.

* Customers need access to the data (purchases history)


Secondary goal: Business intelligence goals
This are things that make optimize the business

For example:

Average sales per hour/day/month/year

Most sold product h/m/y

Highest invoices

Customer insights

== Relational storage vs Non-relational storage

This is a snapshot of the data we are going to deal with:


image::https://user-images.githubusercontent.com/63274055/167107915-9c1db302-e446-433f-816f-ed4eb681dbc6.png[width=600]

At first glance, I would say that this is a very structured and simple dataset, it makes sense to store it in a structured way, in a relational database. It would take maximum four tables. Let#s say, Customer, Invoice, Stock and a table to help us to create a relationship between Invoice number + Stock codeID. This database will then extract the data, transform it and send it to us by a result.

On a small scale this setup is totally valid, however if we think of having millions of customers, all processing the data, specially when writing the data as invoices are being created all the time with many writes, needing to go fast, with every import indexes would need to be created, this setup would create extra levels of complexity and slowing down the platform.

What I rather use here is a Non-relational wide column store.

== Platform Design

image::https://user-images.githubusercontent.com/63274055/167119770-8d0d992a-3627-4a1d-8286-b2686dd64dbc.png[width=800]


This is the blueprint of what I am going to build.

The Client is the way to actually process the data, which is going to be on my pc with the CSV file stored.there.

Python will take lines out of the CSV file and send them over to the Connector, the API Gateway as a JSON string.

Then we have Kinesis as a Buffer.

For the  Processing Framework I will use Lambda functions and when doing batch processing I'll use CloudWatch to actually trigger the Lambdas.

As storage, I will opt for multiple options:

Raw data is going to live on S3, everything coming in will be sent to S3. Data will be in a Non-SQL store, DynamoDB, the I will use Redshift as my data Warehouse.

In terms of how I am going to visualize the data I opted for Tableau, with an API specially for visualization.

=== Client

Setting up a Client to send the data:

image::https://user-images.githubusercontent.com/63274055/167126585-fea1d925-62df-42bf-97cd-b9a62a4efa9e.png[width=800]

=== Connect

image::https://user-images.githubusercontent.com/63274055/167153132-d42160c2-3238-4512-991c-c911275bc5e3.png[width=800]

In this Connect face we have the Client which is sending data to our API Gateway that is hosting a URL.

When the data is sent to the API, in the background is living a Lambda function that is going to get triggered by the API Gateway and that is processing the JSON we have and it is going to access a Buffer and send it into Kinesis. 

=== Buffer

image::https://user-images.githubusercontent.com/63274055/167160182-a5625423-90fc-49e2-b624-24b3315769b9.png[width=800]

Here we have Kinesis in the middle, a message queue. The way message queues work is that we have Producers and we have Consumers. The Producer is going to send data into the Message Queue and the Consumer is going to take data out of the Message Queue. In my case, the Producer is the Lambda Function that sits behind the API Gateway. 

The Lambda Function is getting triggered by the API Gateway and it is going to send or produce the data string for Kinesis. So every message or every transaction or row that is coming in from the dataset that has been processed into JSON is going to be added into Kinesis. Finally we can have either a Lambda Function or we could have a Kinesis Firehose which will take the data back out.

=== Process

image::https://user-images.githubusercontent.com/63274055/167196783-afbbef55-fe3e-4019-a9da-6de1ea264188.png[width=800]


The are two ways of processing data. We can either do stream processing or batch processing.
When we think about Stream Processing we have a Source (Kinesis) and its sending data into 
Processing (Lambda Function which is triggered by a new Kinesis record), once new data is written 
into Kinesis the Lambda Function automatically runs and processes the data right away and puts it 
into a destination.

Batch Processing in another hand starts with the Scheduler. Once the data is put into the data 
source (S3 Kinesis). A Scheduler(CloudWatch) is going to start and activate the Processing 
(Lambda Function). The Processing is going to connect to the data source, pull the data,
process the data then write the data to the destination.

=== Storage

image::https://user-images.githubusercontent.com/63274055/167223592-e36eb180-ea7e-4d74-9f7f-1acde7339a87.png[width=600]

I am going to use S3 file storage, which is very simple to use and I will use it for bulk imports when we have transactions coming in as a file then we upload them to S3, triggering a bulk import then write it directly into DynamoDB and the Redshift Data Warehouse. I will use DynamoDB wide column store which is a non-relational database. 

I will use it because for this business application I want to simulate in this project, in my opinion, it does not make sense to use a SQL database. DynamoDB is for the backend where the transactions are going to happen and where the "customer" or user is going to visualize their data, in this case their invoices and the invoice detail.

This project has a primary use case which is for the actual business and the user, then I have an Analytics use case which is secondary. For this analytics layer which is on top of the actual database layer, Redshift will be my data warehouse. 

This is to simulate what big companies do in this situations, having databases all over the company and they want to use analytics on top of this databases, so they pull data from this databases up into an analytics layer (into a data warehouse) where thy have then a storage and a way to accessing the data. This is not for transactions or business related, it is actually a duplication of the data.

=== Visualize

* API's
** Access for Apps, UIs.
** Execute queries and transaction

* Tableau
** Business Intelligence tool
** Installed on pc
** Connects to Redshift

== Planning the Data Pipelines

Pipelines are the main vehicle that makes data science happen. Pipelines make sure that the data flows from the moment data gets in (Ingestion) through the whole platform and make the data accessible to other systems, data analysts, data scientist and internal users.

=== Data Ingestion Pipeline for high ingestion loads

image::https://user-images.githubusercontent.com/63274055/167260893-0038d3ed-83af-425e-a66c-ed5b188c04bd.png[width=600]

I am going to create a Client that simulates the streaming of the data. It has the CSV file from Kaggle.
Then my aim is to send in rows from the CSV file as JSON into the url of the API gateway.

The Client is going to basically take each row of the CSV file, convert it into a JSON object, then adds that JSON object (each row of the CSV file) into the body of the HTTP post, then post it to the API.

Once it is on the API gateway, where the URL is hosted, there is going to be a Lambda Function triggered, then this Lambda Function is going to take the body of tke post and it is going to write it into Kinesis.

=== Stream to S3 Raw Storage Pipeline

image::https://user-images.githubusercontent.com/63274055/167261665-f99cff9d-fe81-40ad-b7ac-34f6e2a0020c.png[width=600]

The idea here is to take the data that is in Kinesis Stream, trigger the Lambda with it each and every time something is inserted into the Kinesis Data Stream then puts the data into a S3 bucket as a file.


=== Stream to DynamoDB Pipeline

image::https://user-images.githubusercontent.com/63274055/167268945-5d7be067-3a2f-4388-b92e-9d5539e780a5.png[width=600]

This pipeline is going to stream the data from Kinesis into DynamoDB.

Having the data buffered into Kinesis, the data needs to be processed and sent to DynamoDB. This pipeline is similar to the previous one, I am going to take the Kinesis stream data (messages), trigger a Lambda Function with it each time it comes in, and this Lambda Function then re-processes and re-formats the data according to how I am going to then query it later.

=== Visualization Pipeline API


image::https://user-images.githubusercontent.com/63274055/167271582-c47599d6-1227-455f-b5f3-761de17f3d8f.png[width=600]


Here I will build an API to query items from an invoice.

On the left side the data resides in DynamoDB Invoices Table and on the right side is where the Client sits. The Client could be for example an app, an UI. So the Client makes a call onto the invoice API and tells it basically that is has an invoice number, now tell me all the items. Then the request gets processed by a Lambda Function which is going to look into DynamoDB, take the data and return it to the Client. The way I will do this is by sending the invoice number in the request parameter.

=== Visualization Pipeline Redshift Data Warehouse

image::https://user-images.githubusercontent.com/63274055/167290091-e1b30c46-3111-40b7-9d66-31be449e6d28.png[width=600]

AWS Kinesis Firehose Delivery Stream is ideal to process data in Kinesis. Firehose Delivery Stream basically docks on the Kinesis Data Stream and they can process the data further. Then the Delivery Stream automatically delivers the data to the Redshift Data Warehouse with not extra processing.

So I am going to connect the Firehose to the Stream and the Firehose then writes the data into an intermediate S3 bucket. Once there in S3 Kinesis Firehose will trigger a copy to Redshift Function and that copies the complete content of the files into the Redshift table. Once the data is on Redshift we can connect to Tableau and access the data. 

=== Batch Processing Pipeline

image::https://user-images.githubusercontent.com/63274055/167305636-a5c61110-548c-4cd2-b02e-40e2cb75ceaa.png[width=600]

To be able to handle those cases when hundreds of Gb of data need to be uploaded, feeding this data to the Stream Processing would be inefficient. Instead we can do this with a bulk import. So we would simply store the file into S3, CloudWatch would trigger a Lambda Function which takes the data and write it into the DynamoDB tables and into Redshift.

== Data Ingestion Pipeline

=== Creating the Lambda Function for API:

Creating a Lambda Function with a new role to give it permission to write into Kinesis.

image::https://user-images.githubusercontent.com/63274055/167586007-b2bc62cf-5ea0-45eb-99fb-fb93253d92d8.png[width=600]


=== Creating API Gateway

image::https://user-images.githubusercontent.com/63274055/167670433-6d03d9ab-5085-40be-b63a-ffa46db139cb.png[width=600]


On my AWS account, I selected API Gateway => Create API => Rest API => Build

Then select settings:

* Choose the protocol: REST
* Create new API: New API
* Settings:
** API Name: myapi
** Endpoint Type: Regional (Regional API are deployed in the current AWS region)

Then Save API




image::https://user-images.githubusercontent.com/63274055/167672051-17849b3d-09bf-441c-8ff5-befbb5ce2671.png[width=600]
====
''''
====



====
''''
====


//{script-url}[Solutions script only]

xref:Lab-xxxx[Top Section]

xref:Last-section[Bottom section]

//bla bla blafootnote:[{fn-xxx}]


////
.Unordered list title
* gagagagagaga
** gagagatrtrtrzezeze
*** zreu fhjdf hdrfj 
*** hfbvbbvtrtrttrhc
* rtez uezrue rjek  

.Ordered list title
. rwieuzr skjdhf
.. weurthg kjhfdsk skhjdgf
. djhfgsk skjdhfgs 
.. lksjhfgkls ljdfhgkd
... kjhfks sldfkjsdlk




[,sql]
----
----



[NOTE]
====
A sample note admonition.
====
 
TIP: It works!
 
IMPORTANT: Asciidoctor is awesome, don't forget!
 
CAUTION: Don't forget to add the `...-caption` document attributes in the header of the document on GitHub.
 
WARNING: You have no reason not to use Asciidoctor.

bla bla bla the 1NF or first normal form.footnote:[{1nf}]Then wen bla bla


====
- [*] checked
- [x] also checked
- [ ] not checked
-     normal list item
====
[horizontal]
CPU:: The brain of the computer.
Hard drive:: Permanent storage for operating system and/or user files.
RAM:: Temporarily stores information the CPU uses during operation.






bold *constrained* & **un**constrained

italic _constrained_ & __un__constrained

bold italic *_constrained_* & **__un__**constrained

monospace `constrained` & ``un``constrained

monospace bold `*constrained*` & ``**un**``constrained

monospace italic `_constrained_` & ``__un__``constrained

monospace bold italic `*_constrained_*` & ``**__un__**``constrained

////
