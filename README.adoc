= Data engineering with AWS - Final Project
:stylesheet: boot-darkly.css
:linkcss: boot-darkly.css
:image-url-ironhack: https://user-images.githubusercontent.com/23629340/40541063-a07a0a8a-601a-11e8-91b5-2f13e4e6b441.png
:my-name: Jorge Castro DAPT NOV2021
:description:
:script-url: 
//:fn-xxx: Add the explanation foot note here bla bla
:toc:
:toc-title: Lab description: In this lab, you will....
:toc-placement!:
:toclevels: 5
ifdef::env-github[]
:sectnums:
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
:experimental:
:table-caption!:
:example-caption!:
:figure-caption!:
:idprefix:
:idseparator: -
:linkattrs:
:fontawesome-ref: http://fortawesome.github.io/Font-Awesome
:icon-inline: {user-ref}/#inline-icons
:icon-attribute: {user-ref}/#size-rotate-and-flip
:video-ref: {user-ref}/#video
:checklist-ref: {user-ref}/#checklists
:list-marker: {user-ref}/#custom-markers
:list-number: {user-ref}/#numbering-styles
:imagesdir-ref: {user-ref}/#imagesdir
:image-attributes: {user-ref}/#put-images-in-their-place
:toc-ref: {user-ref}/#table-of-contents
:para-ref: {user-ref}/#paragraph
:literal-ref: {user-ref}/#literal-text-and-blocks
:admon-ref: {user-ref}/#admonition
:bold-ref: {user-ref}/#bold-and-italic
:quote-ref: {user-ref}/#quotation-marks-and-apostrophes
:sub-ref: {user-ref}/#subscript-and-superscript
:mono-ref: {user-ref}/#monospace
:css-ref: {user-ref}/#custom-styling-with-attributes
:pass-ref: {user-ref}/#passthrough-macros
endif::[]
ifndef::env-github[]
:imagesdir: ./
endif::[]

image::{image-url-ironhack}[width=70]

{my-name}


                                                     
====
''''
====
toc::[]

{description}


= About the project:

In essence, I want to mimic a e-Commerce platform to make sure that once the data is coming in, the data is getting processed, stored then be accessible to be used and visualized.

I am going to build:  

* a data ingestion pipeline, with API, Lambda and Kenisis.
* a streaming pipeline to raw data on S3.
* a storage element to DynamoDB (NoSQL) and a visualization of this data with an API.
* Streaming data to a data warehouse with Redshift.
* a batch processing pipeline for bulk import.

== Dataset

https://www.kaggle.com/datasets/carrie1/ecommerce-data[E-Commerce Data]

Actual transactions from UK retailer

From https://www.kaggle.com/datasets/carrie1/ecommerce-data

CSV files

Customer ID
Invoice number 
Description of the item
invoice date
Unit price and quantity field

540.000 rows



Main goal
: User / Business

* Transactions will be started in the e-Commerce platform, then this transactions must be, stored in a single location.

* Customers need access to the data (purchases history)


Secondary goal: Business intelligence goals
This are things that make optimize the business

For example:

Average sales per hour/day/month/year

Most sold product h/m/y

Highest invoices

Customer insights

== Relational storage vs Non-relational storage

This is a snapshot of the data we are going to deal with:


image::https://user-images.githubusercontent.com/63274055/167107915-9c1db302-e446-433f-816f-ed4eb681dbc6.png[width=600]

At first glance, I would say that this is a very structured and simple dataset, it makes sense to store it in a structured way, in a relational database. It would take maximum four tables. Let#s say, Customer, Invoice, Stock and a table to help us to create a relationship between Invoice number + Stock codeID. This database will then extract the data, transform it and send it to us by a result.

On a small scale this setup is totally valid, however if we think of having millions of customers, all processing the data, specially when writing the data as invoices are being created all the time with many writes, needing to go fast, with every import indexes would need to be created, this setup would create extra levels of complexity and slowing down the platform.

What I rather use here is a Non-relational wide column store.

== Platform Design

image::https://user-images.githubusercontent.com/63274055/167119770-8d0d992a-3627-4a1d-8286-b2686dd64dbc.png[width=600]


This is the blueprint of what I am going to build.

The Client is the way to actually process the data, which is going to be on my pc with the CSV file stored.there.

Python will take lines out of the CSV file and send them over to the Connector, the API Gateway as a JSON string.

Then we have Kinesis as a Buffer.

For the  Processing Framework I will use Lambda functions and when doing batch processing I'll use CloudWatch to actually trigger the Lambdas.

As storage, I will opt for multiple options:

Raw data is going to live on S3, everything coming in will be sent to S3. Data will be in a Non-SQL store, DynamoDB, the I will use Redshift as my data Warehouse.

In terms of how I am going to visualize the data I opted for Tableau, with an API specially for visualization.

=== Client

Setting up a Client to send the data:

image::https://user-images.githubusercontent.com/63274055/167126585-fea1d925-62df-42bf-97cd-b9a62a4efa9e.png[width=800]

=== Connect

image::https://user-images.githubusercontent.com/63274055/167153132-d42160c2-3238-4512-991c-c911275bc5e3.png[width=800]

In this Connect face we have the Client which is sending data to our API Gateway that is hosting a URL.

When the data is sent to the API, in the background is living a Lambda function that is going to get triggered by the API Gateway and that is processing the JSON we have and it is going to access a Buffer and send it into Kinesis. 

=== Buffer

image::https://user-images.githubusercontent.com/63274055/167160182-a5625423-90fc-49e2-b624-24b3315769b9.png[width=800]

Here we have Kinesis in the middle, a message queue. The way message queues work is that we have Producers and we have Consumers. The Producer is going to send data into the Message Queue and the Consumer is going to take data out of the Message Queue. In my case, the Producer is the Lambda Function that sits behind the API Gateway. 

The Lambda Function is getting triggered by the API Gateway and it is going to send or produce the data string for Kinesis. So every message or every transaction or row that is coming in from the dataset that has been processed into JSON is going to be added into Kinesis. Finally we can have either a Lambda Function or we could have a Kinesis Firehose which will take the data back out.
====
''''
====



====
''''
====


//{script-url}[Solutions script only]

xref:Lab-xxxx[Top Section]

xref:Last-section[Bottom section]

//bla bla blafootnote:[{fn-xxx}]


////
.Unordered list title
* gagagagagaga
** gagagatrtrtrzezeze
*** zreu fhjdf hdrfj 
*** hfbvbbvtrtrttrhc
* rtez uezrue rjek  

.Ordered list title
. rwieuzr skjdhf
.. weurthg kjhfdsk skhjdgf
. djhfgsk skjdhfgs 
.. lksjhfgkls ljdfhgkd
... kjhfks sldfkjsdlk




[,sql]
----
----



[NOTE]
====
A sample note admonition.
====
 
TIP: It works!
 
IMPORTANT: Asciidoctor is awesome, don't forget!
 
CAUTION: Don't forget to add the `...-caption` document attributes in the header of the document on GitHub.
 
WARNING: You have no reason not to use Asciidoctor.

bla bla bla the 1NF or first normal form.footnote:[{1nf}]Then wen bla bla


====
- [*] checked
- [x] also checked
- [ ] not checked
-     normal list item
====
[horizontal]
CPU:: The brain of the computer.
Hard drive:: Permanent storage for operating system and/or user files.
RAM:: Temporarily stores information the CPU uses during operation.






bold *constrained* & **un**constrained

italic _constrained_ & __un__constrained

bold italic *_constrained_* & **__un__**constrained

monospace `constrained` & ``un``constrained

monospace bold `*constrained*` & ``**un**``constrained

monospace italic `_constrained_` & ``__un__``constrained

monospace bold italic `*_constrained_*` & ``**__un__**``constrained

////
